{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ucimlrepo\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, recall_score\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "\n",
    "# Qiskit imports\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit.providers.fake_provider import GenericBackendV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                           DATA FETCHING\n",
    "##############################################################################\n",
    "\n",
    "# 1) Fetch Statlog (German Credit) dataset\n",
    "statlog_german_credit_data = fetch_ucirepo(id=144)\n",
    "\n",
    "X = statlog_german_credit_data.data.features\n",
    "y = statlog_german_credit_data.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                             TRAIN/TEST SPLIT\n",
    "##############################################################################\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                     SEPARATE NUMERIC & CATEGORICAL FEATURES\n",
    "##############################################################################\n",
    "\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "numerical_features = X.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "X_train_cat = X_train_raw[categorical_features]\n",
    "X_train_num = X_train_raw[numerical_features]\n",
    "\n",
    "X_test_cat = X_test_raw[categorical_features]\n",
    "X_test_num = X_test_raw[numerical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                   FIT TRANSFORMERS ON TRAIN, TRANSFORM TEST\n",
    "##############################################################################\n",
    "\n",
    "# 1) One-hot encode categorical columns (fit on train, transform on test)\n",
    "onehot = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "onehot.fit(X_train_cat)\n",
    "\n",
    "X_train_cat_enc = onehot.transform(X_train_cat)\n",
    "X_test_cat_enc  = onehot.transform(X_test_cat)\n",
    "\n",
    "# 2) Standard scale numerical columns (fit on train, transform on test)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_num)\n",
    "\n",
    "X_train_num_scaled = scaler.transform(X_train_num)\n",
    "X_test_num_scaled  = scaler.transform(X_test_num)\n",
    "\n",
    "# 3) Combine the preprocessed numerical + categorical features\n",
    "X_train_processed = np.hstack([X_train_num_scaled, X_train_cat_enc])\n",
    "X_test_processed  = np.hstack([X_test_num_scaled,  X_test_cat_enc])\n",
    "\n",
    "# 4) PCA on the combined training data, then transform the test set\n",
    "pca = PCA(n_components=8)\n",
    "pca.fit(X_train_processed)\n",
    "\n",
    "X_train_reduced = pca.transform(X_train_processed)\n",
    "X_test_reduced  = pca.transform(X_test_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                CLASS IMBALANCE HANDLING (SMOTE + UNDER-SAMPLING)\n",
    "##############################################################################\n",
    "\n",
    "# SMOTE oversamples the minority class up to 80% of the majority\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.8)\n",
    "# Slightly undersample the majority to 90%\n",
    "rus = RandomUnderSampler(random_state=42, sampling_strategy=0.9)\n",
    "\n",
    "resampler = imbPipeline([('smote', smote), ('rus', rus)])\n",
    "X_train_resampled, y_train_resampled = resampler.fit_resample(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                      QUANTUM RESERVOIR COMPUTING CLASS\n",
    "##############################################################################\n",
    "\n",
    "class QuantumReservoirComputing:\n",
    "    \"\"\"\n",
    "    A simplified quantum reservoir computing approach that:\n",
    "      - Creates an n-qubit circuit.\n",
    "      - Encodes classical data via rotation gates.\n",
    "      - Applies reservoir layers (ring entanglement + random rotations).\n",
    "      - Measures all qubits, converting bitstring distribution to a feature vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_qubits=8, n_layers=3):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        # Use a generic fake backend with a ring coupling map\n",
    "        self.backend = GenericBackendV2(\n",
    "            num_qubits=n_qubits,\n",
    "            basis_gates=['rx', 'ry', 'rz', 'cx', 'h'],\n",
    "            coupling_map=[[i, i + 1] for i in range(n_qubits - 1)],\n",
    "            seed=42\n",
    "        )\n",
    "    \n",
    "    def create_quantum_circuit(self, features):\n",
    "        qc = QuantumCircuit(self.n_qubits)\n",
    "        \n",
    "        # 1) Initial Hadamard layer\n",
    "        for i in range(self.n_qubits):\n",
    "            qc.h(i)\n",
    "        \n",
    "        # 2) Encode features with parametric rotations, \n",
    "        #    but remove chain CNOT to avoid redundant entanglement\n",
    "        for i in range(min(len(features), self.n_qubits)):\n",
    "            angle = np.arctan(np.clip(features[i], -10, 10))\n",
    "            qc.ry(angle, i)\n",
    "            qc.rz(angle * np.pi, i)\n",
    "        \n",
    "        # 3) Reservoir layers: ring entanglement + random rotations\n",
    "        for layer in range(self.n_layers):\n",
    "            # Ring entanglement\n",
    "            for i in range(self.n_qubits):\n",
    "                qc.cx(i, (i + 1) % self.n_qubits)\n",
    "            \n",
    "            # Random transformations\n",
    "            for i in range(self.n_qubits):\n",
    "                rx_angle = np.sin(layer * np.pi / self.n_layers + i * 2 * np.pi / self.n_qubits)\n",
    "                rz_angle = np.cos(layer * np.pi / self.n_layers + i * 2 * np.pi / self.n_qubits)\n",
    "                qc.rx(rx_angle, i)\n",
    "                qc.rz(rz_angle, i)\n",
    "        \n",
    "        qc.measure_all()\n",
    "        return qc\n",
    "    \n",
    "    def get_quantum_features(self, circuit):\n",
    "        # Transpile and run on the fake backend\n",
    "        transpiled_circuit = transpile(circuit, self.backend, optimization_level=3)\n",
    "        job = self.backend.run(transpiled_circuit, shots=3000)\n",
    "        result = job.result()\n",
    "        counts = result.get_counts()\n",
    "        \n",
    "        # Convert bitstring frequencies into a feature vector\n",
    "        feature_vector = np.zeros(2 ** self.n_qubits)\n",
    "        total_shots = sum(counts.values())\n",
    "        \n",
    "        for bitstring, count in counts.items():\n",
    "            index = int(bitstring, 2)\n",
    "            # Nonlinear scaling\n",
    "            feature_vector[index] = np.tanh((count / total_shots) * 3)\n",
    "            \n",
    "        return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                       HYBRID QUANTUM-CLASSICAL CLASSIFIER\n",
    "##############################################################################\n",
    "\n",
    "class HybridClassifier:\n",
    "    \"\"\"\n",
    "    Uses the QuantumReservoirComputing class to produce quantum feature vectors,\n",
    "    then trains a LogisticRegression model on top of those features.\n",
    "    \"\"\"\n",
    "    def __init__(self, quantum_reservoir, classifier):\n",
    "        self.quantum_reservoir = quantum_reservoir\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        quantum_features = []\n",
    "        print(\"Generating quantum features for training...\")\n",
    "        \n",
    "        for i, sample in enumerate(X):\n",
    "            if i % 10 == 0:\n",
    "                print(f\"  Processing sample {i}/{len(X)}\")\n",
    "            circuit = self.quantum_reservoir.create_quantum_circuit(sample)\n",
    "            features = self.quantum_reservoir.get_quantum_features(circuit)\n",
    "            quantum_features.append(features)\n",
    "        \n",
    "        quantum_features = np.array(quantum_features)\n",
    "        print(\"Training classifier...\")\n",
    "        self.classifier.fit(quantum_features, np.ravel(y))  \n",
    "    \n",
    "    def predict(self, X):\n",
    "        quantum_features = []\n",
    "        for sample in X:\n",
    "            circuit = self.quantum_reservoir.create_quantum_circuit(sample)\n",
    "            features = self.quantum_reservoir.get_quantum_features(circuit)\n",
    "            quantum_features.append(features)\n",
    "        quantum_features = np.array(quantum_features)\n",
    "        return self.classifier.predict(quantum_features)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        quantum_features = []\n",
    "        for sample in X:\n",
    "            circuit = self.quantum_reservoir.create_quantum_circuit(sample)\n",
    "            features = self.quantum_reservoir.get_quantum_features(circuit)\n",
    "            quantum_features.append(features)\n",
    "        quantum_features = np.array(quantum_features)\n",
    "        return self.classifier.predict_proba(quantum_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Quantum Reservoir Computing...\n",
      "Starting model training...\n",
      "Generating quantum features for training...\n",
      "  Processing sample 0/945\n",
      "  Processing sample 10/945\n",
      "  Processing sample 20/945\n",
      "  Processing sample 30/945\n",
      "  Processing sample 40/945\n",
      "  Processing sample 50/945\n",
      "  Processing sample 60/945\n",
      "  Processing sample 70/945\n",
      "  Processing sample 80/945\n",
      "  Processing sample 90/945\n",
      "  Processing sample 100/945\n",
      "  Processing sample 110/945\n",
      "  Processing sample 120/945\n",
      "  Processing sample 130/945\n",
      "  Processing sample 140/945\n",
      "  Processing sample 150/945\n",
      "  Processing sample 160/945\n",
      "  Processing sample 170/945\n",
      "  Processing sample 180/945\n",
      "  Processing sample 190/945\n",
      "  Processing sample 200/945\n",
      "  Processing sample 210/945\n",
      "  Processing sample 220/945\n",
      "  Processing sample 230/945\n",
      "  Processing sample 240/945\n",
      "  Processing sample 250/945\n",
      "  Processing sample 260/945\n",
      "  Processing sample 270/945\n",
      "  Processing sample 280/945\n",
      "  Processing sample 290/945\n",
      "  Processing sample 300/945\n",
      "  Processing sample 310/945\n",
      "  Processing sample 320/945\n",
      "  Processing sample 330/945\n",
      "  Processing sample 340/945\n",
      "  Processing sample 350/945\n",
      "  Processing sample 360/945\n",
      "  Processing sample 370/945\n",
      "  Processing sample 380/945\n",
      "  Processing sample 390/945\n",
      "  Processing sample 400/945\n",
      "  Processing sample 410/945\n",
      "  Processing sample 420/945\n",
      "  Processing sample 430/945\n",
      "  Processing sample 440/945\n",
      "  Processing sample 450/945\n",
      "  Processing sample 460/945\n",
      "  Processing sample 470/945\n",
      "  Processing sample 480/945\n",
      "  Processing sample 490/945\n",
      "  Processing sample 500/945\n",
      "  Processing sample 510/945\n",
      "  Processing sample 520/945\n",
      "  Processing sample 530/945\n",
      "  Processing sample 540/945\n",
      "  Processing sample 550/945\n",
      "  Processing sample 560/945\n",
      "  Processing sample 570/945\n",
      "  Processing sample 580/945\n",
      "  Processing sample 590/945\n",
      "  Processing sample 600/945\n",
      "  Processing sample 610/945\n",
      "  Processing sample 620/945\n",
      "  Processing sample 630/945\n",
      "  Processing sample 640/945\n",
      "  Processing sample 650/945\n",
      "  Processing sample 660/945\n",
      "  Processing sample 670/945\n",
      "  Processing sample 680/945\n",
      "  Processing sample 690/945\n",
      "  Processing sample 700/945\n",
      "  Processing sample 710/945\n",
      "  Processing sample 720/945\n",
      "  Processing sample 730/945\n",
      "  Processing sample 740/945\n",
      "  Processing sample 750/945\n",
      "  Processing sample 760/945\n",
      "  Processing sample 770/945\n",
      "  Processing sample 780/945\n",
      "  Processing sample 790/945\n",
      "  Processing sample 800/945\n",
      "  Processing sample 810/945\n",
      "  Processing sample 820/945\n",
      "  Processing sample 830/945\n",
      "  Processing sample 840/945\n",
      "  Processing sample 850/945\n",
      "  Processing sample 860/945\n",
      "  Processing sample 870/945\n",
      "  Processing sample 880/945\n",
      "  Processing sample 890/945\n",
      "  Processing sample 900/945\n",
      "  Processing sample 910/945\n",
      "  Processing sample 920/945\n",
      "  Processing sample 930/945\n",
      "  Processing sample 940/945\n",
      "Training classifier...\n",
      "\n",
      "Evaluating with default threshold=0.5 ...\n",
      "Confusion Matrix (default threshold):\n",
      "[[86 54]\n",
      " [30 30]]\n",
      "\n",
      "Classification Report (default threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.61      0.67       140\n",
      "           2       0.36      0.50      0.42        60\n",
      "\n",
      "    accuracy                           0.58       200\n",
      "   macro avg       0.55      0.56      0.54       200\n",
      "weighted avg       0.63      0.58      0.60       200\n",
      "\n",
      "Minority Class Recall (default thr): 0.500\n",
      "\n",
      "Tuning threshold to target recall ~0.5 for minority class=2...\n",
      "Best threshold for recall ~0.5 is: 0.50 (difference=0.000)\n",
      "\n",
      "Confusion Matrix (tuned threshold):\n",
      "[[91 49]\n",
      " [30 30]]\n",
      "\n",
      "Classification Report (tuned threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.65      0.70       140\n",
      "           2       0.38      0.50      0.43        60\n",
      "\n",
      "    accuracy                           0.60       200\n",
      "   macro avg       0.57      0.57      0.56       200\n",
      "weighted avg       0.64      0.60      0.62       200\n",
      "\n",
      "Minority Class Recall (tuned thr): 0.500\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#                         MODEL TRAINING & THRESHOLD TUNING\n",
    "##############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing Quantum Reservoir Computing...\")\n",
    "    qrc = QuantumReservoirComputing(n_qubits=8, n_layers=3)\n",
    "    \n",
    "    # Use a balanced class weight to give the minority some emphasis\n",
    "    balanced_lr = LogisticRegression(\n",
    "        C=0.3,           # Reduced C => stronger regularization\n",
    "        max_iter=3000,\n",
    "        class_weight='balanced',\n",
    "        solver='saga',\n",
    "        penalty='elasticnet',\n",
    "        l1_ratio=0.2\n",
    "    )\n",
    "    \n",
    "    # Create hybrid quantum-classical model\n",
    "    model = HybridClassifier(qrc, balanced_lr)\n",
    "    \n",
    "    print(\"Starting model training...\")\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1) Standard predictions with built-in threshold=0.5\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\nEvaluating with default threshold=0.5 ...\")\n",
    "    y_pred_default = model.predict(X_test_reduced)\n",
    "\n",
    "    print(\"Confusion Matrix (default threshold):\")\n",
    "    print(confusion_matrix(y_test, y_pred_default))\n",
    "    print(\"\\nClassification Report (default threshold):\")\n",
    "    print(classification_report(y_test, y_pred_default))\n",
    "\n",
    "    minority_recall_default = recall_score(y_test, y_pred_default, pos_label=2)\n",
    "    print(f\"Minority Class Recall (default thr): {minority_recall_default:.3f}\")\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2) Threshold tuning to reach ~0.5 recall for class=2\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\nTuning threshold to target recall ~0.5 for minority class=2...\")\n",
    "    \n",
    "    classes_ = model.classifier.classes_\n",
    "    idx_for_2 = np.where(classes_ == 2)[0][0]\n",
    "\n",
    "    y_test_probs = model.predict_proba(X_test_reduced)[:, idx_for_2]\n",
    "\n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "    target_recall = 0.50\n",
    "    best_threshold = 0.5\n",
    "    best_diff = 1.0\n",
    "\n",
    "    for t in thresholds:\n",
    "        # Predict class=2 if probability >= t\n",
    "        y_pred_t = np.where(y_test_probs >= t, 2, 1)\n",
    "        recall_t = recall_score(y_test, y_pred_t, pos_label=2)\n",
    "        \n",
    "        diff = abs(recall_t - target_recall)\n",
    "        if diff < best_diff:\n",
    "            best_diff = diff\n",
    "            best_threshold = t\n",
    "\n",
    "    print(f\"Best threshold for recall ~0.5 is: {best_threshold:.2f} (difference={best_diff:.3f})\")\n",
    "\n",
    "    y_pred_tuned = np.where(y_test_probs >= best_threshold, 2, 1)\n",
    "    print(\"\\nConfusion Matrix (tuned threshold):\")\n",
    "    print(confusion_matrix(y_test, y_pred_tuned))\n",
    "\n",
    "    print(\"\\nClassification Report (tuned threshold):\")\n",
    "    print(classification_report(y_test, y_pred_tuned))\n",
    "\n",
    "    minority_recall_tuned = recall_score(y_test, y_pred_tuned, pos_label=2)\n",
    "    print(f\"Minority Class Recall (tuned thr): {minority_recall_tuned:.3f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
