{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# UCI data fetcher\n",
    "import ucimlrepo\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Scikit-Learn & Imbalanced-Learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score\n",
    "\n",
    "# PennyLane\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp  # PennyLane-compatible NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                           DATA FETCHING\n",
    "##############################################################################\n",
    "\n",
    "# 1) Fetch Statlog (German Credit) dataset\n",
    "statlog_german_credit_data = fetch_ucirepo(id=144)\n",
    "\n",
    "# X: Features, y: Targets (1=Good, 2=Bad)\n",
    "X = statlog_german_credit_data.data.features\n",
    "y = statlog_german_credit_data.data.targets  # 1=Good, 2=Bad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                           TRAIN/TEST SPLIT\n",
    "##############################################################################\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                SEPARATE NUMERIC & CATEGORICAL FEATURES\n",
    "##############################################################################\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "numerical_features   = X.select_dtypes(exclude=[\"object\"]).columns\n",
    "\n",
    "X_train_cat = X_train_raw[categorical_features]\n",
    "X_train_num = X_train_raw[numerical_features]\n",
    "\n",
    "X_test_cat  = X_test_raw[categorical_features]\n",
    "X_test_num  = X_test_raw[numerical_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                   FIT TRANSFORMERS ON TRAIN, TRANSFORM TEST\n",
    "##############################################################################\n",
    "\n",
    "# 1) One-hot encode categorical columns\n",
    "onehot = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "onehot.fit(X_train_cat)\n",
    "\n",
    "X_train_cat_enc = onehot.transform(X_train_cat).toarray()\n",
    "X_test_cat_enc  = onehot.transform(X_test_cat).toarray()\n",
    "\n",
    "# 2) Standard scale numerical columns\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_num)\n",
    "\n",
    "X_train_num_scaled = scaler.transform(X_train_num)\n",
    "X_test_num_scaled  = scaler.transform(X_test_num)\n",
    "\n",
    "# 3) Combine numerical + categorical features\n",
    "X_train_processed = np.hstack([X_train_num_scaled, X_train_cat_enc])\n",
    "X_test_processed  = np.hstack([X_test_num_scaled,  X_test_cat_enc])\n",
    "\n",
    "# 4) PCA on the combined training data -> 4 components\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(X_train_processed)\n",
    "\n",
    "X_train_reduced = pca.transform(X_train_processed)\n",
    "X_test_reduced  = pca.transform(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#               CLASS IMBALANCE HANDLING (SMOTE + UNDERSAMPLING)\n",
    "##############################################################################\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.8)  # oversample minority to ~80%\n",
    "rus   = RandomUnderSampler(random_state=42, sampling_strategy=0.9)  # undersample majority to ~90%\n",
    "\n",
    "resampler = imbPipeline([(\"smote\", smote), (\"rus\", rus)])\n",
    "X_train_res, y_train_res = resampler.fit_resample(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                           PENNYLANE QNODE SETUP\n",
    "##############################################################################\n",
    "\n",
    "NUM_QUBITS = 4\n",
    "dev = qml.device(\"default.qubit\", wires=NUM_QUBITS)\n",
    "\n",
    "@qml.qnode(dev, interface=\"autograd\")\n",
    "def circuit(emb_params, res_params, x):\n",
    "    \"\"\"\n",
    "    Parametric circuit that:\n",
    "      1) Applies an initial Hadamard to each qubit.\n",
    "      2) Embeds classical features x\n",
    "      3) Applies reservoir layers (res_params) WITHOUT CNOT entanglement\n",
    "      4) Returns <Z> on the last qubit.\n",
    "    \"\"\"\n",
    "    # 1) Initial Hadamard\n",
    "    for w in range(NUM_QUBITS):\n",
    "        qml.Hadamard(wires=w)\n",
    "\n",
    "    # 2) Embedding: single-qubit rotations only\n",
    "    for i in range(NUM_QUBITS):\n",
    "        angle = x[i] if i < len(x) else 0.0\n",
    "        qml.RY(angle, wires=i)\n",
    "\n",
    "    # 3) Reservoir layers (NO entanglement here)\n",
    "    L = len(res_params)\n",
    "    for layer in range(L):\n",
    "        # Only single-qubit rotations in each layer\n",
    "        for i in range(NUM_QUBITS):\n",
    "            rz_angle, rx_angle = res_params[layer][i]\n",
    "            qml.RZ(rz_angle, wires=i)\n",
    "            qml.RX(rx_angle, wires=i)\n",
    "\n",
    "    # 4) Measure <Z> on the last qubit\n",
    "    return qml.expval(qml.PauliZ(NUM_QUBITS - 1))\n",
    "\n",
    "def model(emb_params, res_params, x):\n",
    "    \"\"\"\n",
    "    Convert the circuit's <Z> to a probability for class=2: p_2 = 0.5*(1 - <Z>).\n",
    "    \"\"\"\n",
    "    raw_out = circuit(emb_params, res_params, x)  # in [-1,1]\n",
    "    return 0.5 * (1.0 - raw_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                 BINARY CROSS ENTROPY + COST FUNCTION\n",
    "##############################################################################\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    BCE for one sample: map {1->0, 2->1} => BCE.\n",
    "    \"\"\"\n",
    "    y_bin = 1 if y_true == 2 else 0\n",
    "    eps = 1e-8\n",
    "    y_clamped = pnp.clip(y_pred, eps, 1 - eps)\n",
    "    return -(y_bin * pnp.log(y_clamped) + (1 - y_bin) * pnp.log(1 - y_clamped))\n",
    "\n",
    "def total_cost(emb_params, res_params, X_batch, y_batch):\n",
    "    \"\"\"\n",
    "    Mean BCE over a batch of samples.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    for x, y_ in zip(X_batch, y_batch):\n",
    "        y_p = model(emb_params, res_params, x)\n",
    "        loss += binary_cross_entropy(y_, y_p)\n",
    "    return loss / len(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                   COST WRAPPER FOR PENNYLANE GRAD\n",
    "##############################################################################\n",
    "def cost_fn(params, X_batch, y_batch):\n",
    "    \"\"\"\n",
    "    Single-argument cost function. 'params' is a tuple: (emb_params, res_params).\n",
    "    \"\"\"\n",
    "    emb, res = params\n",
    "    return total_cost(emb, res, X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                       PARAM INITIALIZATION\n",
    "##############################################################################\n",
    "\n",
    "L = 2  # number of reservoir layers\n",
    "emb_params = pnp.zeros(NUM_QUBITS, requires_grad=True)\n",
    "res_params = 0.01 * pnp.random.randn(L, NUM_QUBITS, 2, requires_grad=True)\n",
    "\n",
    "# We'll store them in a single tuple\n",
    "params = (emb_params, res_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Loss = 0.686260\n",
      "Epoch 10/30, Loss = 0.684791\n",
      "Epoch 15/30, Loss = 0.684017\n",
      "Epoch 20/30, Loss = 0.684111\n",
      "Epoch 25/30, Loss = 0.686323\n",
      "Epoch 30/30, Loss = 0.681407\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#                          TRAINING LOOP\n",
    "##############################################################################\n",
    "\n",
    "X_train_pnp = pnp.array(X_train_res, requires_grad=False)\n",
    "y_train_pnp = pnp.array(y_train_res, requires_grad=False)\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 16\n",
    "lr = 0.1\n",
    "\n",
    "num_batches = int(np.ceil(len(X_train_pnp) / batch_size))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle indices\n",
    "    idx = np.random.permutation(len(X_train_pnp))\n",
    "    X_train_pnp = X_train_pnp[idx]\n",
    "    y_train_pnp = y_train_pnp[idx]\n",
    "\n",
    "    avg_loss = 0.0\n",
    "    for b in range(num_batches):\n",
    "        start = b * batch_size\n",
    "        end   = start + batch_size\n",
    "        X_batch = X_train_pnp[start:end]\n",
    "        y_batch = y_train_pnp[start:end]\n",
    "\n",
    "        # 1) Compute gradient\n",
    "        grad_fn = qml.grad(lambda p: cost_fn(p, X_batch, y_batch), argnum=0)\n",
    "        grads = grad_fn(params)\n",
    "\n",
    "        # 2) Gradient descent update\n",
    "        emb_new = params[0] - lr * grads[0]\n",
    "        res_new = params[1] - lr * grads[1]\n",
    "        params  = (emb_new, res_new)\n",
    "\n",
    "        # 3) Accumulate cost\n",
    "        loss_val = cost_fn(params, X_batch, y_batch)\n",
    "        avg_loss += loss_val\n",
    "\n",
    "    avg_loss /= num_batches\n",
    "    if (epoch + 1) % 5 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss = {avg_loss:0.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation with threshold=0.5 ---\n",
      "Confusion Matrix:\n",
      "[[ 34 106]\n",
      " [ 14  46]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.24      0.36       140\n",
      "           2       0.30      0.77      0.43        60\n",
      "\n",
      "    accuracy                           0.40       200\n",
      "   macro avg       0.51      0.50      0.40       200\n",
      "weighted avg       0.59      0.40      0.38       200\n",
      "\n",
      "Minority Class (label=2) Recall: 0.767\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#                         EVALUATE ON TEST SET\n",
    "##############################################################################\n",
    "\n",
    "def predict_class(params, x, threshold=0.3):\n",
    "    \"\"\"\n",
    "    If p_2 >= threshold => class=2 else class=1\n",
    "    \"\"\"\n",
    "    emb, res = params\n",
    "    p_2 = model(emb, res, x)\n",
    "    return 2 if p_2 >= threshold else 1\n",
    "\n",
    "emb_final, res_final = params\n",
    "\n",
    "y_pred = [predict_class(params, x, threshold=0.5) for x in X_test_reduced]\n",
    "\n",
    "print(\"\\n--- Evaluation with threshold=0.5 ---\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "minority_recall = recall_score(y_test, y_pred, pos_label=2)\n",
    "print(f\"Minority Class (label=2) Recall: {minority_recall:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
